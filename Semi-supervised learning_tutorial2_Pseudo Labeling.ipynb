{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a0f1299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: easydict in c:\\users\\dmqa\\anaconda3\\envs\\minjung\\lib\\site-packages (1.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19dbf997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import easydict\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from augmentation import RandAugmentCIFAR\n",
    "from models import WideResNet, ModelEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d587dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"seed\" : 0,\n",
    "    \"gpu\": 0,\n",
    "    \"start_step\" : 0,\n",
    "    \"total_steps\" : 30000,\n",
    "    \"eval_step\" : 100,\n",
    "    \"lambda_u\" : 0.01,\n",
    "    \n",
    "    # for data\n",
    "    \"data_path\" : \"./data\", \n",
    "    \"num_labeled\" : 5000,# number of labeled data\n",
    "    \"num_classes\" : 10, # number of classes\n",
    "    \"resize\" : 32, # resize image\n",
    "    \"batch_size\" : 64,\n",
    "    \"mu\" : 1, # coefficient of unlabeled batch size,\n",
    "    \n",
    "    # for WideResNet model\n",
    "    \"depth\" : 28,\n",
    "    \"widen_factor\" : 2,\n",
    "    \"teacher_dropout\" : 0, # dropout on last dense layer of teacher model\n",
    "    \"student_dropout\" : 0, # dropout on last dense layer of student model\n",
    "    \n",
    "    # for optimizing\n",
    "    \"teacher_lr\" : 0.01, # train learning rate of teacher model\n",
    "    \"student_lr\" : 0.01, # train learning rate of student model\n",
    "    \"momentum\" : 0.9, # SGD Momentum\n",
    "    \"nesterov\" : True, # use nesterov\n",
    "    \"weight_decay\" : 0, # train weight decay\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24096281",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device('cuda', args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c0695a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3f9bcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "base_dataset = datasets.CIFAR10(args.data_path, train=True, download=True)\n",
    "test_dataset = datasets.CIFAR10(args.data_path, train=False, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e156a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_u_split(args, labels):\n",
    "    label_per_class = args.num_labeled // args.num_classes\n",
    "    labels = np.array(labels)\n",
    "    labeled_idx = []\n",
    "    \n",
    "    unlabeled_idx = np.array(range(len(labels))) # unlabeled data: all training data\n",
    "    for i in range(args.num_classes):\n",
    "        idx = np.where(labels == i)[0]\n",
    "        idx = np.random.choice(idx, label_per_class, False)\n",
    "        labeled_idx.extend(idx)\n",
    "    labeled_idx = np.array(labeled_idx)\n",
    "    np.random.shuffle(labeled_idx)\n",
    "    \n",
    "    # unlabeled_idx = np.array([i for i in unlabeled_idx if i not in labeled_idx])\n",
    "    return labeled_idx, unlabeled_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f705e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_idxs, unlabeled_idxs = l_u_split(args, base_dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8460629",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_mean = (0.491400, 0.482158, 0.4465231)\n",
    "cifar10_std = (0.247032, 0.243485, 0.2615877)\n",
    "\n",
    "transform_labeled = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(size=args.resize,\n",
    "                              padding=int(args.resize * 0.125),\n",
    "                              fill=128,\n",
    "                              padding_mode='constant'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std),\n",
    "    ])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
    "    ])\n",
    "\n",
    "class CustomTransform(object):\n",
    "    def __init__(self, args, mean, std):\n",
    "        n, m = 2, 10\n",
    "        \n",
    "        self.ori = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=args.resize,\n",
    "                                  padding=int(args.resize * 0.125),\n",
    "                                  fill=128,\n",
    "                                  padding_mode='constant')])\n",
    "        \n",
    "        self.aug = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=args.resize,\n",
    "                                  padding=int(args.resize * 0.125),\n",
    "                                  fill=128,\n",
    "                                  padding_mode='constant'),\n",
    "            RandAugmentCIFAR(n=n, m=m)])\n",
    "        \n",
    "        self.normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        ori = self.ori(x)\n",
    "        aug = self.aug(x)\n",
    "        return self.normalize(ori), self.normalize(aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01235e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCIFAR10SSL(datasets.CIFAR10):\n",
    "    def __init__(self, root, indexs, train=True,\n",
    "                 transform=None, target_transform=None, download=False):\n",
    "        super().__init__(root, train=train,\n",
    "                         transform=transform,\n",
    "                         target_transform=target_transform,\n",
    "                         download=download)\n",
    "        if indexs is not None:\n",
    "            self.data = self.data[indexs]\n",
    "            self.targets = np.array(self.targets)[indexs]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03597a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset = CustomCIFAR10SSL(args.data_path, labeled_idxs, train=True, transform=transform_labeled)\n",
    "unlabeled_dataset = CustomCIFAR10SSL(args.data_path, unlabeled_idxs, train=True, \n",
    "                                     transform=CustomTransform(args, mean=cifar10_mean, std=cifar10_std))\n",
    "test_dataset = datasets.CIFAR10(args.data_path, train=False, transform=transform_val, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ca4266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_loader = DataLoader(labeled_dataset, sampler=RandomSampler(labeled_dataset),\n",
    "                            batch_size=args.batch_size, drop_last=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, sampler=RandomSampler(unlabeled_dataset),\n",
    "                              batch_size=args.batch_size * args.mu, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afdf781",
   "metadata": {},
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e399a315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 1.47M\n"
     ]
    }
   ],
   "source": [
    "teacher_model = WideResNet(num_classes=args.num_classes,\n",
    "                           depth=args.depth,\n",
    "                           widen_factor=args.widen_factor,\n",
    "                           dropout=0,\n",
    "                           dense_dropout=args.teacher_dropout)\n",
    "teacher_model.to(args.device)\n",
    "print(f\"Params: {sum(p.numel() for p in teacher_model.parameters())/1e6:.2f}M\")\n",
    "# K킬로 1000, M 메가 100만 million, G 기가 10억 billion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00589c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(teacher_model.parameters(), lr=args.teacher_lr, momentum=args.momentum, nesterov=args.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a79061fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Loss : 1.9969\n",
      "2 Loss : 1.7568\n",
      "3 Loss : 1.6429\n",
      "4 Loss : 1.5526\n",
      "5 Loss : 1.4886\n",
      "6 Loss : 1.4297\n",
      "7 Loss : 1.3584\n",
      "8 Loss : 1.3083\n",
      "9 Loss : 1.2663\n",
      "10 Loss : 1.2027\n",
      "11 Loss : 1.1688\n",
      "12 Loss : 1.1478\n",
      "13 Loss : 1.1140\n",
      "14 Loss : 1.0737\n",
      "15 Loss : 1.0463\n",
      "16 Loss : 1.0108\n",
      "17 Loss : 1.0036\n",
      "18 Loss : 0.9770\n",
      "19 Loss : 0.9250\n",
      "20 Loss : 0.9342\n",
      "21 Loss : 0.9000\n",
      "22 Loss : 0.8721\n",
      "23 Loss : 0.8501\n",
      "24 Loss : 0.8400\n",
      "25 Loss : 0.8084\n",
      "26 Loss : 0.7942\n",
      "27 Loss : 0.7591\n",
      "28 Loss : 0.7505\n",
      "29 Loss : 0.7238\n",
      "30 Loss : 0.7215\n",
      "31 Loss : 0.7000\n",
      "32 Loss : 0.6843\n",
      "33 Loss : 0.6584\n",
      "34 Loss : 0.6646\n",
      "35 Loss : 0.6220\n",
      "36 Loss : 0.6251\n",
      "37 Loss : 0.6028\n",
      "38 Loss : 0.6084\n",
      "39 Loss : 0.5687\n",
      "40 Loss : 0.5522\n",
      "41 Loss : 0.5259\n",
      "42 Loss : 0.5245\n",
      "43 Loss : 0.5137\n",
      "44 Loss : 0.4872\n",
      "45 Loss : 0.4931\n",
      "46 Loss : 0.4537\n",
      "47 Loss : 0.4440\n",
      "48 Loss : 0.4277\n",
      "49 Loss : 0.4479\n",
      "50 Loss : 0.4093\n",
      "51 Loss : 0.4158\n",
      "52 Loss : 0.3942\n",
      "53 Loss : 0.3851\n",
      "54 Loss : 0.3645\n",
      "55 Loss : 0.3498\n",
      "56 Loss : 0.3550\n",
      "57 Loss : 0.3467\n",
      "58 Loss : 0.3091\n",
      "59 Loss : 0.3463\n",
      "60 Loss : 0.3455\n",
      "61 Loss : 0.2917\n",
      "62 Loss : 0.2884\n",
      "63 Loss : 0.2808\n",
      "64 Loss : 0.2654\n",
      "65 Loss : 0.2592\n",
      "66 Loss : 0.2761\n",
      "67 Loss : 0.2509\n",
      "68 Loss : 0.2614\n",
      "69 Loss : 0.2343\n",
      "70 Loss : 0.2302\n",
      "71 Loss : 0.2399\n",
      "72 Loss : 0.2165\n",
      "73 Loss : 0.2157\n",
      "74 Loss : 0.1891\n",
      "75 Loss : 0.2071\n",
      "76 Loss : 0.2118\n",
      "77 Loss : 0.1972\n",
      "78 Loss : 0.1803\n",
      "79 Loss : 0.1782\n",
      "80 Loss : 0.1680\n",
      "81 Loss : 0.1961\n",
      "82 Loss : 0.1729\n",
      "83 Loss : 0.1550\n",
      "84 Loss : 0.1685\n",
      "85 Loss : 0.1643\n",
      "86 Loss : 0.1473\n",
      "87 Loss : 0.1386\n",
      "88 Loss : 0.1281\n",
      "89 Loss : 0.1276\n",
      "90 Loss : 0.1403\n",
      "91 Loss : 0.1208\n",
      "92 Loss : 0.1198\n",
      "93 Loss : 0.1148\n",
      "94 Loss : 0.1185\n",
      "95 Loss : 0.1359\n",
      "96 Loss : 0.1110\n",
      "97 Loss : 0.1162\n",
      "98 Loss : 0.1016\n",
      "99 Loss : 0.1147\n",
      "100 Loss : 0.0964\n",
      "Training complete in 5m 50s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "\n",
    "for epoch in range(100):\n",
    "    # 모델은 training mode로 설정\n",
    "    teacher_model.train()\n",
    "    \n",
    "    running_loss = 0\n",
    "    running_total = 0\n",
    "    \n",
    "    for inputs, targets in labeled_loader:\n",
    "        inputs = inputs.to(args.device)\n",
    "        targets = targets.to(args.device, dtype=torch.long)\n",
    "        \n",
    "        # parameter gradients를 0으로 설정\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        outputs = teacher_model(inputs)\n",
    "        #print(outputs)\n",
    "        #print(targets)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # batch별 loss를 축적함\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_total += inputs.size(0)\n",
    "\n",
    "    # epoch의 loss 도출\n",
    "    epoch_loss = running_loss / running_total\n",
    "    print(f'{epoch+1} Loss : {epoch_loss:.4f}')\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fa964f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Acc: 0.6774\n"
     ]
    }
   ],
   "source": [
    "teacher_model.eval()\n",
    "with torch.no_grad():\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(args.device)\n",
    "        targets = targets.to(args.device, dtype=torch.long)\n",
    "        \n",
    "        # forward\n",
    "        outputs = teacher_model(inputs)\n",
    "        \n",
    "        # output 중 최대값의 위치에 해당하는 class로 예측 수행\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        # batch별 정답 개수를 축적함\n",
    "        corrects += torch.sum(preds == targets.data)\n",
    "        total += targets.size(0)\n",
    "\n",
    "test_acc = corrects.double() / total\n",
    "print('Testing Acc: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74397adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_parameter = teacher_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aac229",
   "metadata": {},
   "source": [
    "# Semi-supervized learning using pseudo labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2c43f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (block1): NetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (convShortcut): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block2): NetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (convShortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block3): NetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (convShortcut): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "  (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  (drop): Dropout(p=0, inplace=False)\n",
       "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model = WideResNet(num_classes=args.num_classes,\n",
    "                           depth=args.depth,\n",
    "                           widen_factor=args.widen_factor,\n",
    "                           dropout=0,\n",
    "                           dense_dropout=args.teacher_dropout)\n",
    "teacher_model.to(args.device)\n",
    "\n",
    "student_model = WideResNet(num_classes=args.num_classes,\n",
    "                           depth=args.depth,\n",
    "                           widen_factor=args.widen_factor,\n",
    "                           dropout=0,\n",
    "                           dense_dropout=args.teacher_dropout)\n",
    "student_model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f79288d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model.load_state_dict(teacher_model_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "914c7739",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_optimizer = optim.SGD(teacher_model.parameters(),\n",
    "                        lr=args.teacher_lr,\n",
    "                        momentum=args.momentum,\n",
    "                        nesterov=args.nesterov)\n",
    "s_optimizer = optim.SGD(student_model.parameters(),\n",
    "                        lr=args.student_lr,\n",
    "                        momentum=args.momentum,\n",
    "                        nesterov=args.nesterov)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0c87e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pseudo_labeling(args, teacher_model, student_model, t_optimizer, s_optimizer, criterion):\n",
    "    since = time.time()\n",
    "    for step in range(args.start_step, args.total_steps):\n",
    "        if step % args.eval_step == 0:\n",
    "            if step != 0:\n",
    "                print('{} Step - Teacher loss: {:.4f} Student loss: {:.4f}\\nl_loss: {:.4f} u_loss: {:.4f}'.format(step, np.mean(t_losses), np.mean(s_losses),\n",
    "                                                                                    np.mean(l_losses), np.mean(u_losses)))\n",
    "                \n",
    "            s_losses = []\n",
    "            t_losses = []\n",
    "            l_losses = []\n",
    "            u_losses = []\n",
    "            \n",
    "        teacher_model.train()\n",
    "        student_model.train()\n",
    "\n",
    "        try:\n",
    "            images_l, targets = labeled_iter.next()\n",
    "        except:\n",
    "            labeled_iter = iter(labeled_loader)\n",
    "            images_l, targets = labeled_iter.next()\n",
    "\n",
    "        try:\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "        except:\n",
    "            unlabeled_iter = iter(unlabeled_loader)\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "\n",
    "        images_l = images_l.to(args.device)\n",
    "        images_uw = images_uw.to(args.device)\n",
    "        images_us = images_us.to(args.device)\n",
    "        targets = targets.to(args.device, dtype=torch.long)\n",
    "\n",
    "        # parameter gradients를 0으로 설정\n",
    "        t_optimizer.zero_grad()\n",
    "        s_optimizer.zero_grad()\n",
    "\n",
    "        # forward teacher model\n",
    "        batch_size = images_l.shape[0]\n",
    "        t_images = torch.cat((images_l, images_uw))\n",
    "        t_logits = teacher_model(t_images)\n",
    "        t_logits_l = t_logits[:batch_size]\n",
    "        t_logits_uw = t_logits[batch_size:]\n",
    "        del t_logits\n",
    "\n",
    "        t_loss_l = criterion(t_logits_l, targets)\n",
    "\n",
    "        # make pseudo label\n",
    "        soft_pseudo_label = torch.softmax(t_logits_uw, dim=-1)\n",
    "        max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1)\n",
    "        #mask = max_probs.ge(args.threshold).float()\n",
    "        \n",
    "        # forward student model\n",
    "        s_images = torch.cat((images_l, images_us))\n",
    "        s_logits = student_model(s_images)\n",
    "        s_logits_l = s_logits[:batch_size]\n",
    "        s_logits_us = s_logits[batch_size:]\n",
    "        del s_logits\n",
    "\n",
    "        s_loss_l = criterion(s_logits_l, targets)\n",
    "        s_loss_u = criterion(s_logits_us, hard_pseudo_label.detach())\n",
    "        #s_loss_u =(-(soft_pseudo_label * torch.log_softmax(s_logits_us, dim=-1)).sum(dim=-1) * mask).mean()\n",
    "        s_loss = s_loss_l + (args.lambda_u * s_loss_u)\n",
    "\n",
    "        # backward\n",
    "        t_loss_l.backward()\n",
    "        t_optimizer.step()\n",
    "        \n",
    "        s_loss.backward()\n",
    "        s_optimizer.step()\n",
    "\n",
    "        s_losses.append(s_loss.item())\n",
    "        t_losses.append(t_loss_l.item())\n",
    "        l_losses.append(s_loss_l.item())\n",
    "        u_losses.append(s_loss_u.item())\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9823575a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Step - Teacher loss: 1.9447 Student loss: 0.0625\n",
      "l_loss: 1.9232 u_loss: 2.1544\n",
      "200 Step - Teacher loss: 1.6967 Student loss: 0.0662\n",
      "l_loss: 1.6770 u_loss: 1.9711\n",
      "300 Step - Teacher loss: 1.5648 Student loss: 0.0860\n",
      "l_loss: 1.5459 u_loss: 1.8891\n",
      "400 Step - Teacher loss: 1.4846 Student loss: 0.0906\n",
      "l_loss: 1.4665 u_loss: 1.8103\n",
      "500 Step - Teacher loss: 1.3872 Student loss: 0.0643\n",
      "l_loss: 1.3700 u_loss: 1.7265\n",
      "600 Step - Teacher loss: 1.3109 Student loss: 0.0838\n",
      "l_loss: 1.2940 u_loss: 1.6907\n",
      "700 Step - Teacher loss: 1.2397 Student loss: 0.0744\n",
      "l_loss: 1.2228 u_loss: 1.6849\n",
      "800 Step - Teacher loss: 1.1722 Student loss: 0.0582\n",
      "l_loss: 1.1560 u_loss: 1.6169\n",
      "900 Step - Teacher loss: 1.1222 Student loss: 0.0656\n",
      "l_loss: 1.1065 u_loss: 1.5739\n",
      "1000 Step - Teacher loss: 1.1049 Student loss: 0.0793\n",
      "l_loss: 1.0895 u_loss: 1.5463\n",
      "1100 Step - Teacher loss: 1.0529 Student loss: 0.0524\n",
      "l_loss: 1.0374 u_loss: 1.5508\n",
      "1200 Step - Teacher loss: 0.9979 Student loss: 0.0613\n",
      "l_loss: 0.9826 u_loss: 1.5296\n",
      "1300 Step - Teacher loss: 0.9748 Student loss: 0.0540\n",
      "l_loss: 0.9598 u_loss: 1.5040\n",
      "1400 Step - Teacher loss: 0.9410 Student loss: 0.0523\n",
      "l_loss: 0.9259 u_loss: 1.5077\n",
      "1500 Step - Teacher loss: 0.9143 Student loss: 0.0396\n",
      "l_loss: 0.8997 u_loss: 1.4665\n",
      "1600 Step - Teacher loss: 0.8733 Student loss: 0.0591\n",
      "l_loss: 0.8585 u_loss: 1.4846\n",
      "1700 Step - Teacher loss: 0.8404 Student loss: 0.0516\n",
      "l_loss: 0.8259 u_loss: 1.4527\n",
      "1800 Step - Teacher loss: 0.8317 Student loss: 0.0433\n",
      "l_loss: 0.8170 u_loss: 1.4705\n",
      "1900 Step - Teacher loss: 0.7672 Student loss: 0.0392\n",
      "l_loss: 0.7529 u_loss: 1.4294\n",
      "2000 Step - Teacher loss: 0.7671 Student loss: 0.0503\n",
      "l_loss: 0.7530 u_loss: 1.4172\n",
      "2100 Step - Teacher loss: 0.7552 Student loss: 0.0669\n",
      "l_loss: 0.7409 u_loss: 1.4331\n",
      "2200 Step - Teacher loss: 0.7152 Student loss: 0.0439\n",
      "l_loss: 0.7010 u_loss: 1.4246\n",
      "2300 Step - Teacher loss: 0.6785 Student loss: 0.0391\n",
      "l_loss: 0.6643 u_loss: 1.4232\n",
      "2400 Step - Teacher loss: 0.6511 Student loss: 0.0350\n",
      "l_loss: 0.6370 u_loss: 1.4062\n",
      "2500 Step - Teacher loss: 0.6542 Student loss: 0.0548\n",
      "l_loss: 0.6399 u_loss: 1.4313\n",
      "2600 Step - Teacher loss: 0.6274 Student loss: 0.0428\n",
      "l_loss: 0.6130 u_loss: 1.4415\n",
      "2700 Step - Teacher loss: 0.5856 Student loss: 0.0545\n",
      "l_loss: 0.5715 u_loss: 1.4129\n",
      "2800 Step - Teacher loss: 0.5956 Student loss: 0.0549\n",
      "l_loss: 0.5811 u_loss: 1.4538\n"
     ]
    }
   ],
   "source": [
    "train_pseudo_labeling(args, teacher_model, student_model, t_optimizer, s_optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c13f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        corrects = 0\n",
    "        total = 0\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(args.device)\n",
    "            targets = targets.to(args.device, dtype=torch.long)\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # output 중 최대값의 위치에 해당하는 class로 예측 수행\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # batch별 정답 개수를 축적함\n",
    "            corrects += torch.sum(preds == targets.data)\n",
    "            total += targets.size(0)\n",
    "\n",
    "    test_acc = corrects.double() / total\n",
    "    print('Testing Acc: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(args, student_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
