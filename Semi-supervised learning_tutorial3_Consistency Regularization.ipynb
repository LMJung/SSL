{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37bbef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import easydict\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from augmentation import RandAugmentCIFAR\n",
    "from models import WideResNet, ModelEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89cab206",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"seed\" : 0,\n",
    "    \"gpu\": 0,\n",
    "    \"start_step\" : 0,\n",
    "    \"total_steps\" : 30000,\n",
    "    \"eval_step\" : 100,\n",
    "    \"lambda_u\" : 1,\n",
    "    \"threshold\" : 0.95,\n",
    "    \"T\" : 0.4,\n",
    "    \n",
    "    # for data\n",
    "    \"data_path\" : \"./data\", \n",
    "    \"num_labeled\" : 5000,# number of labeled data\n",
    "    \"num_classes\" : 10, # number of classes\n",
    "    \"resize\" : 32, # resize image\n",
    "    \"batch_size\" : 64,\n",
    "    \"mu\" : 1, # coefficient of unlabeled batch size,\n",
    "    \n",
    "    # for WideResNet model\n",
    "    \"depth\" : 28,\n",
    "    \"widen_factor\" : 2,\n",
    "    \"teacher_dropout\" : 0, # dropout on last dense layer of teacher model\n",
    "    \"student_dropout\" : 0, # dropout on last dense layer of student model\n",
    "    \n",
    "    # for optimizing\n",
    "    \"lr\" : 0.01, # train learning rate of model\n",
    "    \"momentum\" : 0.9, # SGD Momentum\n",
    "    \"nesterov\" : True, # use nesterov\n",
    "    \"weight_decay\" : 0, # train weight decay\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a79296c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device('cuda', args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3850e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888cbb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "base_dataset = datasets.CIFAR10(args.data_path, train=True, download=True)\n",
    "test_dataset = datasets.CIFAR10(args.data_path, train=False, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ad4476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_u_split(args, labels):\n",
    "    label_per_class = args.num_labeled // args.num_classes\n",
    "    labels = np.array(labels)\n",
    "    labeled_idx = []\n",
    "    \n",
    "    unlabeled_idx = np.array(range(len(labels))) # unlabeled data: all training data\n",
    "    for i in range(args.num_classes):\n",
    "        idx = np.where(labels == i)[0]\n",
    "        idx = np.random.choice(idx, label_per_class, False)\n",
    "        labeled_idx.extend(idx)\n",
    "    labeled_idx = np.array(labeled_idx)\n",
    "    np.random.shuffle(labeled_idx)\n",
    "    \n",
    "    # unlabeled_idx = np.array([i for i in unlabeled_idx if i not in labeled_idx])\n",
    "    return labeled_idx, unlabeled_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c4c6371",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_idxs, unlabeled_idxs = l_u_split(args, base_dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ec4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_mean = (0.491400, 0.482158, 0.4465231)\n",
    "cifar10_std = (0.247032, 0.243485, 0.2615877)\n",
    "\n",
    "transform_labeled = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(size=args.resize,\n",
    "                              padding=int(args.resize * 0.125),\n",
    "                              fill=128,\n",
    "                              padding_mode='constant'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std),\n",
    "    ])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
    "    ])\n",
    "\n",
    "class CustomTransform(object):\n",
    "    def __init__(self, args, mean, std):\n",
    "        n, m = 2, 10\n",
    "        \n",
    "        self.ori = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=args.resize,\n",
    "                                  padding=int(args.resize * 0.125),\n",
    "                                  fill=128,\n",
    "                                  padding_mode='constant')])\n",
    "        \n",
    "        self.aug = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=args.resize,\n",
    "                                  padding=int(args.resize * 0.125),\n",
    "                                  fill=128,\n",
    "                                  padding_mode='constant'),\n",
    "            RandAugmentCIFAR(n=n, m=m)])\n",
    "        \n",
    "        self.normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        ori = self.ori(x)\n",
    "        aug = self.aug(x)\n",
    "        return self.normalize(ori), self.normalize(aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bd8ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCIFAR10SSL(datasets.CIFAR10):\n",
    "    def __init__(self, root, indexs, train=True,\n",
    "                 transform=None, target_transform=None, download=False):\n",
    "        super().__init__(root, train=train,\n",
    "                         transform=transform,\n",
    "                         target_transform=target_transform,\n",
    "                         download=download)\n",
    "        if indexs is not None:\n",
    "            self.data = self.data[indexs]\n",
    "            self.targets = np.array(self.targets)[indexs]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed828c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset = CustomCIFAR10SSL(args.data_path, labeled_idxs, train=True, transform=transform_labeled)\n",
    "unlabeled_dataset = CustomCIFAR10SSL(args.data_path, unlabeled_idxs, train=True, \n",
    "                                     transform=CustomTransform(args, mean=cifar10_mean, std=cifar10_std))\n",
    "test_dataset = datasets.CIFAR10(args.data_path, train=False, transform=transform_val, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efbf8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_loader = DataLoader(labeled_dataset, sampler=RandomSampler(labeled_dataset),\n",
    "                            batch_size=args.batch_size, drop_last=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, sampler=RandomSampler(unlabeled_dataset),\n",
    "                              batch_size=args.batch_size * args.mu, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767b68f5",
   "metadata": {},
   "source": [
    "# Semi-supervized learning using consistency regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb2ae980",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideResNet(num_classes=args.num_classes,\n",
    "                   depth=args.depth,\n",
    "                   widen_factor=args.widen_factor,\n",
    "                   dropout=0,\n",
    "                   dense_dropout=args.teacher_dropout)\n",
    "model.to(args.device)\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=args.lr,\n",
    "                      momentum=args.momentum,\n",
    "                      nesterov=args.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d917910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_consistency_regularization(args, model, optimizer):\n",
    "    since = time.time()\n",
    "    for step in range(args.start_step, args.total_steps):\n",
    "        if step % args.eval_step == 0:\n",
    "            if step != 0:\n",
    "                print('{} Step - loss: {:.4f} cross entropy : {:.4f} consistency reg : {:.4f}'.format(step,\n",
    "                                                                                                      np.mean(losses), \n",
    "                                                                                                      np.mean(ce_losses), \n",
    "                                                                                                      np.mean(cr_losses)))\n",
    "        \n",
    "            losses = []\n",
    "            ce_losses = []\n",
    "            cr_losses = []\n",
    "            \n",
    "        model.train()\n",
    "\n",
    "        try:\n",
    "            images_l, targets = labeled_iter.next()\n",
    "        except:\n",
    "            labeled_iter = iter(labeled_loader)\n",
    "            images_l, targets = labeled_iter.next()\n",
    "\n",
    "        try:\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "        except:\n",
    "            unlabeled_iter = iter(unlabeled_loader)\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "\n",
    "        images_l = images_l.to(args.device)\n",
    "        images_uw = images_uw.to(args.device)\n",
    "        images_us = images_us.to(args.device)\n",
    "        targets = targets.to(args.device, dtype=torch.long)\n",
    "\n",
    "        # parameter gradients를 0으로 설정\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward model\n",
    "        batch_size = images_l.shape[0]\n",
    "        images = torch.cat((images_l, images_uw, images_us))\n",
    "        logits = model(images)\n",
    "        logits_l = logits[:batch_size]\n",
    "        logits_uw, logits_us = logits[batch_size:].chunk(2)\n",
    "        del logits\n",
    "\n",
    "        loss_l = F.cross_entropy(logits_l, targets, reduction='mean')\n",
    "\n",
    "        # make pseudo label\n",
    "        #soft_pseudo_label = torch.softmax(logits_uw.detach(), dim=-1)\n",
    "        soft_pseudo_label = torch.softmax(logits_uw.detach()/args.T, dim=-1)\n",
    "        max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1)\n",
    "        mask = max_probs.ge(args.threshold).float()\n",
    "        \n",
    "        loss_u = (-(soft_pseudo_label * torch.log_softmax(logits_us, dim=-1)).sum(dim=-1) * mask).mean()\n",
    "        #loss_u = (((soft_pseudo_label - torch.log_softmax(logits_us, dim=-1))**2).sum(dim=-1) * mask).mean()\n",
    "        loss = loss_l + args.lambda_u * loss_u\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        ce_losses.append(loss_l.item())\n",
    "        cr_losses.append(loss_u.item())\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f417d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Step - loss: 1.9512 cross entropy : 1.9443 consistency reg : 0.0069\n",
      "200 Step - loss: 1.7324 cross entropy : 1.7047 consistency reg : 0.0276\n",
      "300 Step - loss: 1.6480 cross entropy : 1.5600 consistency reg : 0.0880\n",
      "400 Step - loss: 1.6118 cross entropy : 1.4799 consistency reg : 0.1319\n",
      "500 Step - loss: 1.5802 cross entropy : 1.4132 consistency reg : 0.1670\n",
      "600 Step - loss: 1.5534 cross entropy : 1.3322 consistency reg : 0.2212\n",
      "700 Step - loss: 1.5380 cross entropy : 1.2771 consistency reg : 0.2608\n",
      "800 Step - loss: 1.4476 cross entropy : 1.1947 consistency reg : 0.2529\n",
      "900 Step - loss: 1.4341 cross entropy : 1.1529 consistency reg : 0.2812\n",
      "1000 Step - loss: 1.4122 cross entropy : 1.1139 consistency reg : 0.2983\n",
      "1100 Step - loss: 1.3550 cross entropy : 1.0533 consistency reg : 0.3017\n",
      "1200 Step - loss: 1.3582 cross entropy : 1.0363 consistency reg : 0.3219\n",
      "1300 Step - loss: 1.3243 cross entropy : 0.9896 consistency reg : 0.3347\n",
      "1400 Step - loss: 1.2835 cross entropy : 0.9549 consistency reg : 0.3286\n",
      "1500 Step - loss: 1.2538 cross entropy : 0.9213 consistency reg : 0.3324\n",
      "1600 Step - loss: 1.2886 cross entropy : 0.9320 consistency reg : 0.3566\n",
      "1700 Step - loss: 1.2177 cross entropy : 0.8579 consistency reg : 0.3598\n",
      "1800 Step - loss: 1.2045 cross entropy : 0.8465 consistency reg : 0.3580\n",
      "1900 Step - loss: 1.1864 cross entropy : 0.8212 consistency reg : 0.3653\n",
      "2000 Step - loss: 1.1523 cross entropy : 0.7899 consistency reg : 0.3624\n",
      "2100 Step - loss: 1.1567 cross entropy : 0.7849 consistency reg : 0.3718\n",
      "2200 Step - loss: 1.1225 cross entropy : 0.7533 consistency reg : 0.3691\n",
      "2300 Step - loss: 1.1268 cross entropy : 0.7487 consistency reg : 0.3782\n",
      "2400 Step - loss: 1.1015 cross entropy : 0.7178 consistency reg : 0.3837\n",
      "2500 Step - loss: 1.0651 cross entropy : 0.6745 consistency reg : 0.3906\n",
      "2600 Step - loss: 1.0292 cross entropy : 0.6509 consistency reg : 0.3783\n",
      "2700 Step - loss: 1.0463 cross entropy : 0.6511 consistency reg : 0.3952\n",
      "2800 Step - loss: 1.0305 cross entropy : 0.6328 consistency reg : 0.3977\n",
      "2900 Step - loss: 0.9982 cross entropy : 0.6012 consistency reg : 0.3970\n",
      "3000 Step - loss: 0.9818 cross entropy : 0.5789 consistency reg : 0.4029\n",
      "3100 Step - loss: 0.9806 cross entropy : 0.5807 consistency reg : 0.3999\n",
      "3200 Step - loss: 0.9496 cross entropy : 0.5515 consistency reg : 0.3982\n",
      "3300 Step - loss: 0.9326 cross entropy : 0.5213 consistency reg : 0.4113\n",
      "3400 Step - loss: 0.9376 cross entropy : 0.5126 consistency reg : 0.4250\n",
      "3500 Step - loss: 0.8936 cross entropy : 0.5076 consistency reg : 0.3860\n",
      "3600 Step - loss: 0.9142 cross entropy : 0.4872 consistency reg : 0.4271\n",
      "3700 Step - loss: 0.8789 cross entropy : 0.4589 consistency reg : 0.4200\n",
      "3800 Step - loss: 0.8922 cross entropy : 0.4753 consistency reg : 0.4168\n",
      "3900 Step - loss: 0.8487 cross entropy : 0.4371 consistency reg : 0.4116\n",
      "4000 Step - loss: 0.8234 cross entropy : 0.4179 consistency reg : 0.4055\n",
      "4100 Step - loss: 0.8148 cross entropy : 0.3984 consistency reg : 0.4164\n",
      "4200 Step - loss: 0.8466 cross entropy : 0.4082 consistency reg : 0.4384\n",
      "4300 Step - loss: 0.8007 cross entropy : 0.3799 consistency reg : 0.4208\n",
      "4400 Step - loss: 0.7858 cross entropy : 0.3728 consistency reg : 0.4131\n",
      "4500 Step - loss: 0.7838 cross entropy : 0.3592 consistency reg : 0.4247\n",
      "4600 Step - loss: 0.7944 cross entropy : 0.3689 consistency reg : 0.4255\n",
      "4700 Step - loss: 0.7833 cross entropy : 0.3465 consistency reg : 0.4368\n",
      "4800 Step - loss: 0.7545 cross entropy : 0.3389 consistency reg : 0.4156\n",
      "4900 Step - loss: 0.7452 cross entropy : 0.3236 consistency reg : 0.4216\n",
      "5000 Step - loss: 0.7445 cross entropy : 0.3090 consistency reg : 0.4355\n",
      "5100 Step - loss: 0.7513 cross entropy : 0.3135 consistency reg : 0.4379\n",
      "5200 Step - loss: 0.7328 cross entropy : 0.2910 consistency reg : 0.4418\n",
      "5300 Step - loss: 0.7335 cross entropy : 0.2940 consistency reg : 0.4396\n",
      "5400 Step - loss: 0.6929 cross entropy : 0.2798 consistency reg : 0.4131\n",
      "5500 Step - loss: 0.6934 cross entropy : 0.2689 consistency reg : 0.4244\n",
      "5600 Step - loss: 0.6867 cross entropy : 0.2575 consistency reg : 0.4292\n",
      "5700 Step - loss: 0.6896 cross entropy : 0.2457 consistency reg : 0.4438\n",
      "5800 Step - loss: 0.6709 cross entropy : 0.2440 consistency reg : 0.4270\n",
      "5900 Step - loss: 0.6774 cross entropy : 0.2417 consistency reg : 0.4356\n"
     ]
    }
   ],
   "source": [
    "train_consistency_regularization(args, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a82a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        corrects = 0\n",
    "        total = 0\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(args.device)\n",
    "            targets = targets.to(args.device, dtype=torch.long)\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # output 중 최대값의 위치에 해당하는 class로 예측 수행\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # batch별 정답 개수를 축적함\n",
    "            corrects += torch.sum(preds == targets.data)\n",
    "            total += targets.size(0)\n",
    "\n",
    "    test_acc = corrects.double() / total\n",
    "    print('Testing Acc: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(args, model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
