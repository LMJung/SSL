{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67587e99",
   "metadata": {},
   "source": [
    "# <Semi-supervised learning tutorial 2 - pseudo labeling>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "039e69bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import easydict# dictionary의 속성을 dot(.)을 사용하여 표기가능\n",
    "from tqdm.auto import tqdm #ipython파일에서 출력을 깔끔하게하기위해 tqdm.tqdm 대신 tqdm.auto.tqdm 또는 tqdm.notebook.tqdm 사용\n",
    "from PIL import Image # PIL(Python Imaging Library)\n",
    "\n",
    "from augmentation import RandAugmentCIFAR # 데이터 증강에 필요한 함수 작성해 모아놓은 augmentation.py 파일\n",
    "from models import WideResNet # 모델 관련 함수 작성해 모아놓은 models.py 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1c32d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"seed\" : 0,\n",
    "    \"gpu\": 0,\n",
    "    \"start_step\" : 0,\n",
    "    \"total_steps\" : 2000, # 300000\n",
    "    \"eval_step\" : 20, # 100\n",
    "    \"lambda_u\" : 1,\n",
    "    \n",
    "    # for supervised learning\n",
    "    \"total_epoch\" : 100,\n",
    "    \n",
    "    # for data\n",
    "    \"data_path\" : \"./data\",\n",
    "    \"num_data\" : 10000, # 50000\n",
    "    \"num_labeled\" : 1000,# 5000 \n",
    "    \"num_classes\" : 10, # number of classes\n",
    "    \"resize\" : 32, # resize image\n",
    "    \"batch_size\" : 64,\n",
    "    \"mu\" : 1, # coefficient of unlabeled batch size,\n",
    "    \n",
    "    # for WideResNet model\n",
    "    \"depth\" : 10, # 기본 28, assert((depth - 4) % 6 == 0) 학습 시간을 줄이기 위해서 모델 크기 줄임\n",
    "    \"widen_factor\" : 1, # 기본 2, , 학습 시간을 줄이기 위해서 모델 크기 줄임\n",
    "    \"teacher_dropout\" : 0, # dropout on last dense layer of teacher model\n",
    "    \"student_dropout\" : 0, # dropout on last dense layer of student model\n",
    "    \n",
    "    # for optimizing\n",
    "    \"teacher_lr\" : 0.01, # train learning rate of teacher model\n",
    "    \"student_lr\" : 0.01, # train learning rate of student model\n",
    "    \"momentum\" : 0.9, # SGD Momentum\n",
    "    \"nesterov\" : True, # use nesterov\n",
    "    \"weight_decay\" : 0.01, # train weight decay\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a3c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device('cuda', args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9184cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f687fd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "base_dataset = datasets.CIFAR10(args.data_path, train=True, download=True)\n",
    "test_dataset = datasets.CIFAR10(args.data_path, train=False, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df47c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_u_split(args, labels):\n",
    "    \n",
    "    label_per_class = args.num_labeled // args.num_classes\n",
    "    num_unlabel_data = ((args.num_data // args.num_classes) - label_per_class) * args.num_classes\n",
    "    # 학습 시간을 줄이기 위해서 데이터 개수를 줄이기 위해서 추가\n",
    "    \n",
    "    print(f'클래스별 labeled data 개수 : {label_per_class}')\n",
    "    print(f'Labeled data 개수 : {label_per_class * args.num_classes}')\n",
    "    print(f'Unlabeled data 개수 : {num_unlabel_data}')\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    labeled_idx = []\n",
    "    \n",
    "    unlabeled_idx = np.array(range(len(labels))) \n",
    "    for i in range(args.num_classes):\n",
    "        idx = np.where(labels == i)[0]\n",
    "        idx = np.random.choice(idx, label_per_class, False)\n",
    "        labeled_idx.extend(idx)\n",
    "    labeled_idx = np.array(labeled_idx)\n",
    "    np.random.shuffle(labeled_idx)\n",
    "    \n",
    "    unlabeled_idx = np.array([i for i in unlabeled_idx if i not in labeled_idx])\n",
    "    np.random.shuffle(unlabeled_idx)\n",
    "    unlabeled_idx = unlabeled_idx[:num_unlabel_data]\n",
    "    \n",
    "    return labeled_idx, unlabeled_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8cecad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스별 labeled data 개수 : 100\n",
      "Labeled data 개수 : 1000\n",
      "Unlabeled data 개수 : 9000\n"
     ]
    }
   ],
   "source": [
    "labeled_idxs, unlabeled_idxs = l_u_split(args, base_dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc55a11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e80bcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unlabeled_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5cc4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_mean = (0.491400, 0.482158, 0.4465231)\n",
    "cifar10_std = (0.247032, 0.243485, 0.2615877)\n",
    "\n",
    "transform_labeled = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(size=args.resize,\n",
    "                              padding=int(args.resize * 0.125),\n",
    "                              fill=128,\n",
    "                              padding_mode='constant'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
    "    ])\n",
    "\n",
    "class CustomTransform(object):\n",
    "    def __init__(self, args, mean, std):\n",
    "        n, m = 2, 10\n",
    "        \n",
    "        self.ori = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=args.resize,\n",
    "                                  padding=int(args.resize * 0.125),\n",
    "                                  fill=128,\n",
    "                                  padding_mode='constant')])\n",
    "        \n",
    "        self.aug = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=args.resize,\n",
    "                                  padding=int(args.resize * 0.125),\n",
    "                                  fill=128,\n",
    "                                  padding_mode='constant'),\n",
    "            RandAugmentCIFAR(n=n, m=m)])\n",
    "        \n",
    "        self.normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        ori = self.ori(x)\n",
    "        aug = self.aug(x)\n",
    "        return self.normalize(ori), self.normalize(aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd179c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCIFAR10SSL(datasets.CIFAR10):\n",
    "    def __init__(self, root, indexs, train=True,\n",
    "                 transform=None, target_transform=None, download=False):\n",
    "        super().__init__(root, train=train,\n",
    "                         transform=transform,\n",
    "                         target_transform=target_transform,\n",
    "                         download=download)\n",
    "        if indexs is not None:\n",
    "            self.data = self.data[indexs]\n",
    "            self.targets = np.array(self.targets)[indexs]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cd15144",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset = CustomCIFAR10SSL(args.data_path, labeled_idxs, train=True, transform=transform_labeled)\n",
    "unlabeled_dataset = CustomCIFAR10SSL(args.data_path, unlabeled_idxs, train=True, \n",
    "                                     transform=CustomTransform(args, mean=cifar10_mean, std=cifar10_std))\n",
    "test_dataset = datasets.CIFAR10(args.data_path, train=False, transform=transform_test, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75c4303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_loader = DataLoader(labeled_dataset, sampler=RandomSampler(labeled_dataset),\n",
    "                            batch_size=args.batch_size, drop_last=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, sampler=RandomSampler(unlabeled_dataset),\n",
    "                              batch_size=args.batch_size * args.mu, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b9bc8",
   "metadata": {},
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a85ca68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 0.08M\n"
     ]
    }
   ],
   "source": [
    "teacher_model = WideResNet(num_classes=args.num_classes,\n",
    "                           depth=args.depth,\n",
    "                           widen_factor=args.widen_factor,\n",
    "                           dropout=0,\n",
    "                           dense_dropout=args.teacher_dropout)\n",
    "teacher_model.to(args.device)\n",
    "print(f\"Params: {sum(p.numel() for p in teacher_model.parameters())/1e6:.2f}M\")\n",
    "# K킬로 1000, M 메가 100만 million, G 기가 10억 billion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "846cc0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(teacher_model.parameters(), lr=args.teacher_lr, momentum=args.momentum, nesterov=args.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a339117c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Loss : 2.3356\n",
      "2 Loss : 2.1040\n",
      "3 Loss : 2.0085\n",
      "4 Loss : 1.9521\n",
      "5 Loss : 1.9039\n",
      "6 Loss : 1.8594\n",
      "7 Loss : 1.8283\n",
      "8 Loss : 1.8181\n",
      "9 Loss : 1.7788\n",
      "10 Loss : 1.7856\n",
      "11 Loss : 1.7563\n",
      "12 Loss : 1.7323\n",
      "13 Loss : 1.7137\n",
      "14 Loss : 1.7024\n",
      "15 Loss : 1.6590\n",
      "16 Loss : 1.6627\n",
      "17 Loss : 1.6569\n",
      "18 Loss : 1.6140\n",
      "19 Loss : 1.6137\n",
      "20 Loss : 1.6192\n",
      "21 Loss : 1.5772\n",
      "22 Loss : 1.5979\n",
      "23 Loss : 1.5628\n",
      "24 Loss : 1.5603\n",
      "25 Loss : 1.5550\n",
      "26 Loss : 1.5562\n",
      "27 Loss : 1.5239\n",
      "28 Loss : 1.5268\n",
      "29 Loss : 1.5118\n",
      "30 Loss : 1.4932\n",
      "31 Loss : 1.4881\n",
      "32 Loss : 1.5004\n",
      "33 Loss : 1.4744\n",
      "34 Loss : 1.4648\n",
      "35 Loss : 1.4302\n",
      "36 Loss : 1.4433\n",
      "37 Loss : 1.4216\n",
      "38 Loss : 1.4106\n",
      "39 Loss : 1.4329\n",
      "40 Loss : 1.4056\n",
      "41 Loss : 1.3836\n",
      "42 Loss : 1.3731\n",
      "43 Loss : 1.3744\n",
      "44 Loss : 1.3725\n",
      "45 Loss : 1.3454\n",
      "46 Loss : 1.3518\n",
      "47 Loss : 1.3360\n",
      "48 Loss : 1.3538\n",
      "49 Loss : 1.2863\n",
      "50 Loss : 1.2770\n",
      "51 Loss : 1.3090\n",
      "52 Loss : 1.2970\n",
      "53 Loss : 1.2912\n",
      "54 Loss : 1.2287\n",
      "55 Loss : 1.2644\n",
      "56 Loss : 1.2571\n",
      "57 Loss : 1.2468\n",
      "58 Loss : 1.2252\n",
      "59 Loss : 1.1935\n",
      "60 Loss : 1.1927\n",
      "61 Loss : 1.1988\n",
      "62 Loss : 1.2495\n",
      "63 Loss : 1.1663\n",
      "64 Loss : 1.1988\n",
      "65 Loss : 1.1678\n",
      "66 Loss : 1.1587\n",
      "67 Loss : 1.1310\n",
      "68 Loss : 1.1675\n",
      "69 Loss : 1.1420\n",
      "70 Loss : 1.1331\n",
      "71 Loss : 1.0826\n",
      "72 Loss : 1.0955\n",
      "73 Loss : 1.0973\n",
      "74 Loss : 1.0993\n",
      "75 Loss : 1.1092\n",
      "76 Loss : 1.0821\n",
      "77 Loss : 1.0946\n",
      "78 Loss : 1.0720\n",
      "79 Loss : 1.0321\n",
      "80 Loss : 1.0517\n",
      "81 Loss : 1.0214\n",
      "82 Loss : 1.0184\n",
      "83 Loss : 1.0201\n",
      "84 Loss : 1.0158\n",
      "85 Loss : 1.0317\n",
      "86 Loss : 1.0269\n",
      "87 Loss : 0.9825\n",
      "88 Loss : 0.9801\n",
      "89 Loss : 0.9997\n",
      "90 Loss : 0.9379\n",
      "91 Loss : 1.0144\n",
      "92 Loss : 0.9353\n",
      "93 Loss : 0.9751\n",
      "94 Loss : 0.9210\n",
      "95 Loss : 0.9429\n",
      "96 Loss : 0.9320\n",
      "97 Loss : 0.9455\n",
      "98 Loss : 0.9079\n",
      "99 Loss : 0.9319\n",
      "100 Loss : 0.9505\n",
      "Training complete in 0m 46s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "\n",
    "for epoch in range(args.total_epoch):\n",
    "    # 모델은 training mode로 설정\n",
    "    teacher_model.train()\n",
    "    \n",
    "    running_loss = 0\n",
    "    running_total = 0\n",
    "    \n",
    "    for inputs, targets in labeled_loader:\n",
    "        inputs = inputs.to(args.device)\n",
    "        targets = targets.to(args.device, dtype=torch.long)\n",
    "        \n",
    "        # parameter gradients를 0으로 설정\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        outputs = teacher_model(inputs)\n",
    "        #print(outputs)\n",
    "        #print(targets)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # batch별 loss를 축적함\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_total += inputs.size(0)\n",
    "\n",
    "    # epoch의 loss 도출\n",
    "    epoch_loss = running_loss / running_total\n",
    "    print(f'{epoch+1} Loss : {epoch_loss:.4f}')\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bbaa1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Acc: 0.3158\n"
     ]
    }
   ],
   "source": [
    "teacher_model.eval()\n",
    "with torch.no_grad():\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(args.device)\n",
    "        targets = targets.to(args.device, dtype=torch.long)\n",
    "        \n",
    "        # forward\n",
    "        outputs = teacher_model(inputs)\n",
    "        \n",
    "        # output 중 최대값의 위치에 해당하는 class로 예측 수행\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        # batch별 정답 개수를 축적함\n",
    "        corrects += torch.sum(preds == targets.data)\n",
    "        total += targets.size(0)\n",
    "\n",
    "test_acc = corrects.double() / total\n",
    "print('Testing Acc: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8568bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_parameter = teacher_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c24b1",
   "metadata": {},
   "source": [
    "# Semi-supervized learning using pseudo labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5a98fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (block1): NetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block2): NetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (convShortcut): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block3): NetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (convShortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "  (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  (drop): Dropout(p=0, inplace=False)\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model = WideResNet(num_classes=args.num_classes,\n",
    "                           depth=args.depth,\n",
    "                           widen_factor=args.widen_factor,\n",
    "                           dropout=0,\n",
    "                           dense_dropout=args.teacher_dropout)\n",
    "teacher_model.to(args.device)\n",
    "\n",
    "student_model = WideResNet(num_classes=args.num_classes,\n",
    "                           depth=args.depth,\n",
    "                           widen_factor=args.widen_factor,\n",
    "                           dropout=0,\n",
    "                           dense_dropout=args.teacher_dropout)\n",
    "student_model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2166cf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model.load_state_dict(teacher_model_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "159709b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_optimizer = optim.SGD(teacher_model.parameters(),\n",
    "                        lr=args.teacher_lr,\n",
    "                        momentum=args.momentum,\n",
    "                        nesterov=args.nesterov)\n",
    "s_optimizer = optim.SGD(student_model.parameters(),\n",
    "                        lr=args.student_lr,\n",
    "                        momentum=args.momentum,\n",
    "                        nesterov=args.nesterov)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b6f782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pseudo_labeling(args, teacher_model, student_model, t_optimizer, s_optimizer, criterion):\n",
    "    since = time.time()\n",
    "    for step in range(args.start_step, args.total_steps):\n",
    "        if step % args.eval_step == 0:\n",
    "            if step != 0:\n",
    "                print('{} Step - Teacher loss: {:.4f} Student loss: {:.4f}\\nl_loss: {:.4f} u_loss: {:.4f}'.format(step, np.mean(t_losses), np.mean(s_losses),\n",
    "                                                                                    np.mean(l_losses), np.mean(u_losses)))\n",
    "                \n",
    "            s_losses = []\n",
    "            t_losses = []\n",
    "            l_losses = []\n",
    "            u_losses = []\n",
    "            \n",
    "        teacher_model.train()\n",
    "        student_model.train()\n",
    "\n",
    "        try:\n",
    "            images_l, targets = labeled_iter.next()\n",
    "        except:\n",
    "            labeled_iter = iter(labeled_loader)\n",
    "            images_l, targets = labeled_iter.next()\n",
    "\n",
    "        try:\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "        except:\n",
    "            unlabeled_iter = iter(unlabeled_loader)\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "\n",
    "        images_l = images_l.to(args.device)\n",
    "        images_uw = images_uw.to(args.device)\n",
    "        images_us = images_us.to(args.device)\n",
    "        targets = targets.to(args.device, dtype=torch.long)\n",
    "\n",
    "        # parameter gradients를 0으로 설정\n",
    "        t_optimizer.zero_grad()\n",
    "        s_optimizer.zero_grad()\n",
    "\n",
    "        # forward teacher model\n",
    "        batch_size = images_l.shape[0]\n",
    "        t_images = torch.cat((images_l, images_uw))\n",
    "        t_logits = teacher_model(t_images)\n",
    "        t_logits_l = t_logits[:batch_size]\n",
    "        t_logits_uw = t_logits[batch_size:]\n",
    "        del t_logits\n",
    "\n",
    "        t_loss_l = criterion(t_logits_l, targets)\n",
    "\n",
    "        # make pseudo label\n",
    "        soft_pseudo_label = torch.softmax(t_logits_uw, dim=-1)\n",
    "        max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1)\n",
    "        \n",
    "        # forward student model\n",
    "        s_images = torch.cat((images_l, images_uw))\n",
    "        s_logits = student_model(s_images)\n",
    "        s_logits_l = s_logits[:batch_size]\n",
    "        s_logits_us = s_logits[batch_size:]\n",
    "        del s_logits\n",
    "\n",
    "        s_loss_l = criterion(s_logits_l, targets)\n",
    "        s_loss_u = criterion(s_logits_us, hard_pseudo_label.detach())\n",
    "        s_loss = s_loss_l + (args.lambda_u * s_loss_u)\n",
    "\n",
    "        # backward\n",
    "        t_loss_l.backward()\n",
    "        t_optimizer.step()\n",
    "        \n",
    "        s_loss.backward()\n",
    "        s_optimizer.step()\n",
    "\n",
    "        s_losses.append(s_loss.item())\n",
    "        t_losses.append(t_loss_l.item())\n",
    "        l_losses.append(s_loss_l.item())\n",
    "        u_losses.append(s_loss_u.item())\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68c7bbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Step - Teacher loss: 0.8793 Student loss: 4.3520\n",
      "l_loss: 2.1902 u_loss: 2.1618\n",
      "40 Step - Teacher loss: 0.8881 Student loss: 3.8720\n",
      "l_loss: 1.9775 u_loss: 1.8945\n",
      "60 Step - Teacher loss: 0.8400 Student loss: 3.6188\n",
      "l_loss: 1.8774 u_loss: 1.7413\n",
      "80 Step - Teacher loss: 0.8156 Student loss: 3.4440\n",
      "l_loss: 1.7887 u_loss: 1.6553\n",
      "100 Step - Teacher loss: 0.8475 Student loss: 3.3086\n",
      "l_loss: 1.7160 u_loss: 1.5926\n",
      "120 Step - Teacher loss: 0.8269 Student loss: 3.1813\n",
      "l_loss: 1.6792 u_loss: 1.5021\n",
      "140 Step - Teacher loss: 0.7983 Student loss: 3.0981\n",
      "l_loss: 1.6346 u_loss: 1.4635\n",
      "160 Step - Teacher loss: 0.8461 Student loss: 3.0499\n",
      "l_loss: 1.6366 u_loss: 1.4134\n",
      "180 Step - Teacher loss: 0.8615 Student loss: 3.0389\n",
      "l_loss: 1.6353 u_loss: 1.4035\n",
      "200 Step - Teacher loss: 0.8049 Student loss: 2.9594\n",
      "l_loss: 1.5775 u_loss: 1.3819\n",
      "220 Step - Teacher loss: 0.7798 Student loss: 2.8510\n",
      "l_loss: 1.5319 u_loss: 1.3191\n",
      "240 Step - Teacher loss: 0.8027 Student loss: 2.8583\n",
      "l_loss: 1.5342 u_loss: 1.3241\n",
      "260 Step - Teacher loss: 0.7821 Student loss: 2.7663\n",
      "l_loss: 1.4911 u_loss: 1.2752\n",
      "280 Step - Teacher loss: 0.7667 Student loss: 2.7191\n",
      "l_loss: 1.4647 u_loss: 1.2544\n",
      "300 Step - Teacher loss: 0.7715 Student loss: 2.6950\n",
      "l_loss: 1.4392 u_loss: 1.2557\n",
      "320 Step - Teacher loss: 0.7450 Student loss: 2.5982\n",
      "l_loss: 1.4382 u_loss: 1.1600\n",
      "340 Step - Teacher loss: 0.7380 Student loss: 2.5648\n",
      "l_loss: 1.3826 u_loss: 1.1822\n",
      "360 Step - Teacher loss: 0.7776 Student loss: 2.5181\n",
      "l_loss: 1.3868 u_loss: 1.1313\n",
      "380 Step - Teacher loss: 0.7490 Student loss: 2.5645\n",
      "l_loss: 1.3807 u_loss: 1.1838\n",
      "400 Step - Teacher loss: 0.7507 Student loss: 2.5229\n",
      "l_loss: 1.3676 u_loss: 1.1553\n",
      "420 Step - Teacher loss: 0.6958 Student loss: 2.4237\n",
      "l_loss: 1.3210 u_loss: 1.1027\n",
      "440 Step - Teacher loss: 0.7299 Student loss: 2.3630\n",
      "l_loss: 1.2901 u_loss: 1.0729\n",
      "460 Step - Teacher loss: 0.6819 Student loss: 2.3616\n",
      "l_loss: 1.2610 u_loss: 1.1006\n",
      "480 Step - Teacher loss: 0.7047 Student loss: 2.4081\n",
      "l_loss: 1.3249 u_loss: 1.0832\n",
      "500 Step - Teacher loss: 0.6822 Student loss: 2.3600\n",
      "l_loss: 1.2782 u_loss: 1.0819\n",
      "520 Step - Teacher loss: 0.7129 Student loss: 2.3032\n",
      "l_loss: 1.2622 u_loss: 1.0410\n",
      "540 Step - Teacher loss: 0.6835 Student loss: 2.3007\n",
      "l_loss: 1.2366 u_loss: 1.0640\n",
      "560 Step - Teacher loss: 0.6693 Student loss: 2.2498\n",
      "l_loss: 1.2338 u_loss: 1.0160\n",
      "580 Step - Teacher loss: 0.6550 Student loss: 2.2518\n",
      "l_loss: 1.2329 u_loss: 1.0189\n",
      "600 Step - Teacher loss: 0.6519 Student loss: 2.2025\n",
      "l_loss: 1.1841 u_loss: 1.0185\n",
      "620 Step - Teacher loss: 0.6367 Student loss: 2.2294\n",
      "l_loss: 1.1956 u_loss: 1.0338\n",
      "640 Step - Teacher loss: 0.6618 Student loss: 2.2336\n",
      "l_loss: 1.2248 u_loss: 1.0088\n",
      "660 Step - Teacher loss: 0.6474 Student loss: 2.1656\n",
      "l_loss: 1.1595 u_loss: 1.0061\n",
      "680 Step - Teacher loss: 0.6304 Student loss: 2.1706\n",
      "l_loss: 1.1799 u_loss: 0.9908\n",
      "700 Step - Teacher loss: 0.6120 Student loss: 2.1038\n",
      "l_loss: 1.1116 u_loss: 0.9923\n",
      "720 Step - Teacher loss: 0.6483 Student loss: 2.1360\n",
      "l_loss: 1.1726 u_loss: 0.9634\n",
      "740 Step - Teacher loss: 0.5961 Student loss: 2.1080\n",
      "l_loss: 1.1375 u_loss: 0.9705\n",
      "760 Step - Teacher loss: 0.5919 Student loss: 2.0636\n",
      "l_loss: 1.1063 u_loss: 0.9573\n",
      "780 Step - Teacher loss: 0.5776 Student loss: 2.0307\n",
      "l_loss: 1.0775 u_loss: 0.9532\n",
      "800 Step - Teacher loss: 0.5627 Student loss: 2.0259\n",
      "l_loss: 1.0633 u_loss: 0.9626\n",
      "820 Step - Teacher loss: 0.5582 Student loss: 2.0810\n",
      "l_loss: 1.1073 u_loss: 0.9736\n",
      "840 Step - Teacher loss: 0.5779 Student loss: 2.0998\n",
      "l_loss: 1.0995 u_loss: 1.0003\n",
      "860 Step - Teacher loss: 0.5623 Student loss: 2.0689\n",
      "l_loss: 1.0823 u_loss: 0.9866\n",
      "880 Step - Teacher loss: 0.5361 Student loss: 1.9941\n",
      "l_loss: 1.0626 u_loss: 0.9315\n",
      "900 Step - Teacher loss: 0.5393 Student loss: 1.9718\n",
      "l_loss: 1.0536 u_loss: 0.9182\n",
      "920 Step - Teacher loss: 0.5432 Student loss: 1.9941\n",
      "l_loss: 1.0460 u_loss: 0.9481\n",
      "940 Step - Teacher loss: 0.5354 Student loss: 2.0058\n",
      "l_loss: 1.0632 u_loss: 0.9425\n",
      "960 Step - Teacher loss: 0.5349 Student loss: 2.0139\n",
      "l_loss: 1.0342 u_loss: 0.9797\n",
      "980 Step - Teacher loss: 0.5050 Student loss: 1.9556\n",
      "l_loss: 1.0268 u_loss: 0.9288\n",
      "1000 Step - Teacher loss: 0.5037 Student loss: 1.9408\n",
      "l_loss: 1.0093 u_loss: 0.9316\n",
      "1020 Step - Teacher loss: 0.5436 Student loss: 2.0375\n",
      "l_loss: 1.0380 u_loss: 0.9994\n",
      "1040 Step - Teacher loss: 0.5005 Student loss: 1.9357\n",
      "l_loss: 0.9792 u_loss: 0.9566\n",
      "1060 Step - Teacher loss: 0.4825 Student loss: 1.9391\n",
      "l_loss: 1.0044 u_loss: 0.9347\n",
      "1080 Step - Teacher loss: 0.5155 Student loss: 1.9401\n",
      "l_loss: 0.9848 u_loss: 0.9553\n",
      "1100 Step - Teacher loss: 0.4517 Student loss: 1.8066\n",
      "l_loss: 0.9292 u_loss: 0.8773\n",
      "1120 Step - Teacher loss: 0.4718 Student loss: 1.9177\n",
      "l_loss: 1.0005 u_loss: 0.9172\n",
      "1140 Step - Teacher loss: 0.4991 Student loss: 1.8742\n",
      "l_loss: 0.9723 u_loss: 0.9019\n",
      "1160 Step - Teacher loss: 0.4242 Student loss: 1.8371\n",
      "l_loss: 0.9293 u_loss: 0.9078\n",
      "1180 Step - Teacher loss: 0.4696 Student loss: 1.9171\n",
      "l_loss: 0.9888 u_loss: 0.9283\n",
      "1200 Step - Teacher loss: 0.4881 Student loss: 1.9254\n",
      "l_loss: 0.9854 u_loss: 0.9400\n",
      "1220 Step - Teacher loss: 0.4515 Student loss: 1.8775\n",
      "l_loss: 0.9646 u_loss: 0.9129\n",
      "1240 Step - Teacher loss: 0.4291 Student loss: 1.8421\n",
      "l_loss: 0.8983 u_loss: 0.9439\n",
      "1260 Step - Teacher loss: 0.4590 Student loss: 1.8188\n",
      "l_loss: 0.9182 u_loss: 0.9006\n",
      "1280 Step - Teacher loss: 0.4548 Student loss: 1.8232\n",
      "l_loss: 0.9180 u_loss: 0.9051\n",
      "1300 Step - Teacher loss: 0.4429 Student loss: 1.9094\n",
      "l_loss: 0.9069 u_loss: 1.0025\n",
      "1320 Step - Teacher loss: 0.4417 Student loss: 1.8214\n",
      "l_loss: 0.9263 u_loss: 0.8951\n",
      "1340 Step - Teacher loss: 0.4158 Student loss: 1.8124\n",
      "l_loss: 0.8829 u_loss: 0.9296\n",
      "1360 Step - Teacher loss: 0.4203 Student loss: 1.8461\n",
      "l_loss: 0.9081 u_loss: 0.9379\n",
      "1380 Step - Teacher loss: 0.4372 Student loss: 1.8431\n",
      "l_loss: 0.9297 u_loss: 0.9134\n",
      "1400 Step - Teacher loss: 0.3936 Student loss: 1.7366\n",
      "l_loss: 0.8534 u_loss: 0.8831\n",
      "1420 Step - Teacher loss: 0.3848 Student loss: 1.7367\n",
      "l_loss: 0.8678 u_loss: 0.8689\n",
      "1440 Step - Teacher loss: 0.4315 Student loss: 1.8132\n",
      "l_loss: 0.8676 u_loss: 0.9455\n",
      "1460 Step - Teacher loss: 0.3812 Student loss: 1.7126\n",
      "l_loss: 0.8390 u_loss: 0.8735\n",
      "1480 Step - Teacher loss: 0.3946 Student loss: 1.7701\n",
      "l_loss: 0.8419 u_loss: 0.9282\n",
      "1500 Step - Teacher loss: 0.3590 Student loss: 1.7038\n",
      "l_loss: 0.8056 u_loss: 0.8982\n",
      "1520 Step - Teacher loss: 0.3387 Student loss: 1.7857\n",
      "l_loss: 0.8523 u_loss: 0.9334\n",
      "1540 Step - Teacher loss: 0.3550 Student loss: 1.7163\n",
      "l_loss: 0.8297 u_loss: 0.8866\n",
      "1560 Step - Teacher loss: 0.3489 Student loss: 1.6850\n",
      "l_loss: 0.8108 u_loss: 0.8743\n",
      "1580 Step - Teacher loss: 0.3371 Student loss: 1.7222\n",
      "l_loss: 0.8122 u_loss: 0.9100\n",
      "1600 Step - Teacher loss: 0.3548 Student loss: 1.6618\n",
      "l_loss: 0.8195 u_loss: 0.8423\n",
      "1620 Step - Teacher loss: 0.3259 Student loss: 1.6439\n",
      "l_loss: 0.8037 u_loss: 0.8401\n",
      "1640 Step - Teacher loss: 0.3182 Student loss: 1.6772\n",
      "l_loss: 0.7955 u_loss: 0.8817\n",
      "1660 Step - Teacher loss: 0.3124 Student loss: 1.7176\n",
      "l_loss: 0.8239 u_loss: 0.8936\n",
      "1680 Step - Teacher loss: 0.3337 Student loss: 1.7304\n",
      "l_loss: 0.8292 u_loss: 0.9012\n",
      "1700 Step - Teacher loss: 0.2958 Student loss: 1.6486\n",
      "l_loss: 0.7825 u_loss: 0.8662\n",
      "1720 Step - Teacher loss: 0.3131 Student loss: 1.6353\n",
      "l_loss: 0.7652 u_loss: 0.8700\n",
      "1740 Step - Teacher loss: 0.3319 Student loss: 1.6890\n",
      "l_loss: 0.7829 u_loss: 0.9061\n",
      "1760 Step - Teacher loss: 0.3667 Student loss: 1.6579\n",
      "l_loss: 0.7907 u_loss: 0.8672\n",
      "1780 Step - Teacher loss: 0.3914 Student loss: 1.6695\n",
      "l_loss: 0.7648 u_loss: 0.9046\n",
      "1800 Step - Teacher loss: 0.3597 Student loss: 1.6889\n",
      "l_loss: 0.7853 u_loss: 0.9036\n",
      "1820 Step - Teacher loss: 0.3204 Student loss: 1.6441\n",
      "l_loss: 0.7466 u_loss: 0.8975\n",
      "1840 Step - Teacher loss: 0.3006 Student loss: 1.6351\n",
      "l_loss: 0.7481 u_loss: 0.8871\n",
      "1860 Step - Teacher loss: 0.2777 Student loss: 1.6338\n",
      "l_loss: 0.7387 u_loss: 0.8950\n",
      "1880 Step - Teacher loss: 0.2722 Student loss: 1.5901\n",
      "l_loss: 0.7267 u_loss: 0.8633\n",
      "1900 Step - Teacher loss: 0.3134 Student loss: 1.6046\n",
      "l_loss: 0.7152 u_loss: 0.8893\n",
      "1920 Step - Teacher loss: 0.2700 Student loss: 1.5502\n",
      "l_loss: 0.7064 u_loss: 0.8438\n",
      "1940 Step - Teacher loss: 0.2538 Student loss: 1.5673\n",
      "l_loss: 0.7114 u_loss: 0.8559\n",
      "1960 Step - Teacher loss: 0.2568 Student loss: 1.6438\n",
      "l_loss: 0.7012 u_loss: 0.9425\n",
      "1980 Step - Teacher loss: 0.2570 Student loss: 1.5795\n",
      "l_loss: 0.7056 u_loss: 0.8739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete in 2m 50s\n"
     ]
    }
   ],
   "source": [
    "train_pseudo_labeling(args, teacher_model, student_model, t_optimizer, s_optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "760d6f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        corrects = 0\n",
    "        total = 0\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(args.device)\n",
    "            targets = targets.to(args.device, dtype=torch.long)\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # output 중 최대값의 위치에 해당하는 class로 예측 수행\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # batch별 정답 개수를 축적함\n",
    "            corrects += torch.sum(preds == targets.data)\n",
    "            total += targets.size(0)\n",
    "\n",
    "    test_acc = corrects.double() / total\n",
    "    print('Testing Acc: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b214a97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Acc: 0.4509\n"
     ]
    }
   ],
   "source": [
    "test(args, student_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
