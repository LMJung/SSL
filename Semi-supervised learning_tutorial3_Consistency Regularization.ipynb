{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d2f04a7",
   "metadata": {},
   "source": [
    "# <Semi-supervised learning tutorial 3 - consistency regularization>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37bbef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import easydict\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from augmentation import RandAugmentCIFAR\n",
    "from models import WideResNet, ModelEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89cab206",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"seed\" : 0,\n",
    "    \"gpu\": 0,\n",
    "    \"start_step\" : 0,\n",
    "    \"total_steps\" : 2000, # 30000\n",
    "    \"eval_step\" : 20, # 100\n",
    "    \"lambda_u\" : 1,\n",
    "    \"threshold\" : 0.95,\n",
    "    \"T\" : 0.6,\n",
    "    \n",
    "    # for data\n",
    "    \"data_path\" : \"./data\",\n",
    "    \"num_data\" : 10000, # 50000\n",
    "    \"num_labeled\" : 1000,# 5000 \n",
    "    \"num_classes\" : 10, # number of classes\n",
    "    \"resize\" : 32, # resize image\n",
    "    \"batch_size\" : 64,\n",
    "    \"mu\" : 1, # coefficient of unlabeled batch size,\n",
    "    \n",
    "    # for WideResNet model\n",
    "    \"depth\" : 10,\n",
    "    \"widen_factor\" : 1,\n",
    "    \"teacher_dropout\" : 0, # dropout on last dense layer of teacher model\n",
    "    \"student_dropout\" : 0, # dropout on last dense layer of student model\n",
    "    \n",
    "    # for optimizing\n",
    "    \"lr\" : 0.01, # train learning rate of model\n",
    "    \"momentum\" : 0.9, # SGD Momentum\n",
    "    \"nesterov\" : True, # use nesterov\n",
    "    \"weight_decay\" : 0.01, # train weight decay\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a79296c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device('cuda', args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3850e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888cbb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "base_dataset = datasets.CIFAR10(args.data_path, train=True, download=True)\n",
    "test_dataset = datasets.CIFAR10(args.data_path, train=False, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ad4476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_u_split(args, labels):\n",
    "    \n",
    "    label_per_class = args.num_labeled // args.num_classes\n",
    "    num_unlabel_data = ((args.num_data // args.num_classes) - label_per_class) * args.num_classes\n",
    "    # 학습 시간을 줄이기 위해서 데이터 개수를 줄이기 위해서 추가\n",
    "    \n",
    "    print(f'클래스별 labeled data 개수 : {label_per_class}')\n",
    "    print(f'Labeled data 개수 : {label_per_class * args.num_classes}')\n",
    "    print(f'Unlabeled data 개수 : {num_unlabel_data}')\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    labeled_idx = []\n",
    "    \n",
    "    unlabeled_idx = np.array(range(len(labels))) \n",
    "    for i in range(args.num_classes):\n",
    "        idx = np.where(labels == i)[0]\n",
    "        idx = np.random.choice(idx, label_per_class, False)\n",
    "        labeled_idx.extend(idx)\n",
    "    labeled_idx = np.array(labeled_idx)\n",
    "    np.random.shuffle(labeled_idx)\n",
    "    \n",
    "    unlabeled_idx = np.array([i for i in unlabeled_idx if i not in labeled_idx])\n",
    "    np.random.shuffle(unlabeled_idx)\n",
    "    unlabeled_idx = unlabeled_idx[:num_unlabel_data]\n",
    "    \n",
    "    return labeled_idx, unlabeled_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c4c6371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스별 labeled data 개수 : 100\n",
      "Labeled data 개수 : 1000\n",
      "Unlabeled data 개수 : 9000\n"
     ]
    }
   ],
   "source": [
    "labeled_idxs, unlabeled_idxs = l_u_split(args, base_dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ec4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_mean = (0.491400, 0.482158, 0.4465231)\n",
    "cifar10_std = (0.247032, 0.243485, 0.2615877)\n",
    "\n",
    "transform_labeled = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(size=args.resize,\n",
    "                              padding=int(args.resize * 0.125),\n",
    "                              fill=128,\n",
    "                              padding_mode='constant'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std),\n",
    "    ])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
    "    ])\n",
    "\n",
    "class CustomTransform(object):\n",
    "    def __init__(self, args, mean, std):\n",
    "        n, m = 2, 10\n",
    "        \n",
    "        self.ori = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=args.resize,\n",
    "                                  padding=int(args.resize * 0.125),\n",
    "                                  fill=128,\n",
    "                                  padding_mode='constant')])\n",
    "        \n",
    "        self.aug = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=args.resize,\n",
    "                                  padding=int(args.resize * 0.125),\n",
    "                                  fill=128,\n",
    "                                  padding_mode='constant'),\n",
    "            RandAugmentCIFAR(n=n, m=m)])\n",
    "        \n",
    "        self.normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        ori = self.ori(x)\n",
    "        aug = self.aug(x)\n",
    "        return self.normalize(ori), self.normalize(aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bd8ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCIFAR10SSL(datasets.CIFAR10):\n",
    "    def __init__(self, root, indexs, train=True,\n",
    "                 transform=None, target_transform=None, download=False):\n",
    "        super().__init__(root, train=train,\n",
    "                         transform=transform,\n",
    "                         target_transform=target_transform,\n",
    "                         download=download)\n",
    "        if indexs is not None:\n",
    "            self.data = self.data[indexs]\n",
    "            self.targets = np.array(self.targets)[indexs]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed828c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset = CustomCIFAR10SSL(args.data_path, labeled_idxs, train=True, transform=transform_labeled)\n",
    "unlabeled_dataset = CustomCIFAR10SSL(args.data_path, unlabeled_idxs, train=True, \n",
    "                                     transform=CustomTransform(args, mean=cifar10_mean, std=cifar10_std))\n",
    "test_dataset = datasets.CIFAR10(args.data_path, train=False, transform=transform_val, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efbf8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_loader = DataLoader(labeled_dataset, sampler=RandomSampler(labeled_dataset),\n",
    "                            batch_size=args.batch_size, drop_last=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, sampler=RandomSampler(unlabeled_dataset),\n",
    "                              batch_size=args.batch_size * args.mu, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767b68f5",
   "metadata": {},
   "source": [
    "# Semi-supervized learning using consistency regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb2ae980",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideResNet(num_classes=args.num_classes,\n",
    "                   depth=args.depth,\n",
    "                   widen_factor=args.widen_factor,\n",
    "                   dropout=0,\n",
    "                   dense_dropout=args.teacher_dropout)\n",
    "model.to(args.device)\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=args.lr,\n",
    "                      momentum=args.momentum,\n",
    "                      nesterov=args.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d917910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_consistency_regularization(args, model, optimizer):\n",
    "    since = time.time()\n",
    "    for step in range(args.start_step, args.total_steps):\n",
    "        if step % args.eval_step == 0:\n",
    "            if step != 0:\n",
    "                print('{} Step - loss: {:.4f} cross entropy : {:.4f} consistency reg : {:.4f}'.format(step,\n",
    "                                                                                                      np.mean(losses), \n",
    "                                                                                                      np.mean(ce_losses), \n",
    "                                                                                                      np.mean(cr_losses)))\n",
    "        \n",
    "            losses = []\n",
    "            ce_losses = []\n",
    "            cr_losses = []\n",
    "            \n",
    "        model.train()\n",
    "\n",
    "        try:\n",
    "            images_l, targets = labeled_iter.next()\n",
    "        except:\n",
    "            labeled_iter = iter(labeled_loader)\n",
    "            images_l, targets = labeled_iter.next()\n",
    "\n",
    "        try:\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "        except:\n",
    "            unlabeled_iter = iter(unlabeled_loader)\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "\n",
    "        images_l = images_l.to(args.device)\n",
    "        images_uw = images_uw.to(args.device)\n",
    "        images_us = images_us.to(args.device)\n",
    "        targets = targets.to(args.device, dtype=torch.long)\n",
    "\n",
    "        # parameter gradients를 0으로 설정\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward model\n",
    "        batch_size = images_l.shape[0]\n",
    "        images = torch.cat((images_l, images_uw, images_us))\n",
    "        logits = model(images)\n",
    "        logits_l = logits[:batch_size]\n",
    "        logits_uw, logits_us = logits[batch_size:].chunk(2)\n",
    "        del logits\n",
    "\n",
    "        loss_l = F.cross_entropy(logits_l, targets, reduction='mean')\n",
    "\n",
    "        # make pseudo label\n",
    "        soft_pseudo_label = torch.softmax(logits_uw.detach()/args.T, dim=-1)\n",
    "        max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1)\n",
    "        mask = max_probs.ge(args.threshold).float()\n",
    "        \n",
    "        loss_u = (-(soft_pseudo_label * torch.log_softmax(logits_us, dim=-1)).sum(dim=-1) * mask).mean()\n",
    "        #loss_u = (((soft_pseudo_label - torch.log_softmax(logits_us, dim=-1))**2).sum(dim=-1) * mask).mean()\n",
    "        loss = loss_l + args.lambda_u * loss_u\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        ce_losses.append(loss_l.item())\n",
    "        cr_losses.append(loss_u.item())\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f417d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Step - loss: 2.2866 cross entropy : 2.2866 consistency reg : 0.0000\n",
      "40 Step - loss: 2.0286 cross entropy : 2.0267 consistency reg : 0.0019\n",
      "60 Step - loss: 1.9455 cross entropy : 1.9455 consistency reg : 0.0000\n",
      "80 Step - loss: 1.9014 cross entropy : 1.8959 consistency reg : 0.0055\n",
      "100 Step - loss: 1.8385 cross entropy : 1.8332 consistency reg : 0.0053\n",
      "120 Step - loss: 1.8338 cross entropy : 1.8327 consistency reg : 0.0011\n",
      "140 Step - loss: 1.7828 cross entropy : 1.7819 consistency reg : 0.0010\n",
      "160 Step - loss: 1.7694 cross entropy : 1.7661 consistency reg : 0.0033\n",
      "180 Step - loss: 1.7532 cross entropy : 1.7494 consistency reg : 0.0037\n",
      "200 Step - loss: 1.7004 cross entropy : 1.6999 consistency reg : 0.0005\n",
      "220 Step - loss: 1.6761 cross entropy : 1.6751 consistency reg : 0.0010\n",
      "240 Step - loss: 1.6914 cross entropy : 1.6893 consistency reg : 0.0021\n",
      "260 Step - loss: 1.6558 cross entropy : 1.6513 consistency reg : 0.0045\n",
      "280 Step - loss: 1.6516 cross entropy : 1.6468 consistency reg : 0.0049\n",
      "300 Step - loss: 1.6257 cross entropy : 1.6213 consistency reg : 0.0043\n",
      "320 Step - loss: 1.5951 cross entropy : 1.5879 consistency reg : 0.0072\n",
      "340 Step - loss: 1.6188 cross entropy : 1.6171 consistency reg : 0.0017\n",
      "360 Step - loss: 1.5632 cross entropy : 1.5600 consistency reg : 0.0032\n",
      "380 Step - loss: 1.5719 cross entropy : 1.5648 consistency reg : 0.0071\n",
      "400 Step - loss: 1.5165 cross entropy : 1.5081 consistency reg : 0.0084\n",
      "420 Step - loss: 1.5290 cross entropy : 1.5214 consistency reg : 0.0076\n",
      "440 Step - loss: 1.5424 cross entropy : 1.5144 consistency reg : 0.0281\n",
      "460 Step - loss: 1.4924 cross entropy : 1.4792 consistency reg : 0.0132\n",
      "480 Step - loss: 1.5195 cross entropy : 1.5037 consistency reg : 0.0158\n",
      "500 Step - loss: 1.4579 cross entropy : 1.4516 consistency reg : 0.0064\n",
      "520 Step - loss: 1.4920 cross entropy : 1.4773 consistency reg : 0.0148\n",
      "540 Step - loss: 1.4769 cross entropy : 1.4442 consistency reg : 0.0327\n",
      "560 Step - loss: 1.4321 cross entropy : 1.4205 consistency reg : 0.0116\n",
      "580 Step - loss: 1.4325 cross entropy : 1.4084 consistency reg : 0.0241\n",
      "600 Step - loss: 1.3814 cross entropy : 1.3590 consistency reg : 0.0224\n",
      "620 Step - loss: 1.4165 cross entropy : 1.3761 consistency reg : 0.0404\n",
      "640 Step - loss: 1.4124 cross entropy : 1.3818 consistency reg : 0.0305\n",
      "660 Step - loss: 1.3732 cross entropy : 1.3370 consistency reg : 0.0362\n",
      "680 Step - loss: 1.3481 cross entropy : 1.3136 consistency reg : 0.0345\n",
      "700 Step - loss: 1.3995 cross entropy : 1.3619 consistency reg : 0.0376\n",
      "720 Step - loss: 1.3544 cross entropy : 1.3007 consistency reg : 0.0537\n",
      "740 Step - loss: 1.3359 cross entropy : 1.2900 consistency reg : 0.0459\n",
      "760 Step - loss: 1.3945 cross entropy : 1.3360 consistency reg : 0.0585\n",
      "780 Step - loss: 1.2791 cross entropy : 1.2330 consistency reg : 0.0461\n",
      "800 Step - loss: 1.3436 cross entropy : 1.2726 consistency reg : 0.0710\n",
      "820 Step - loss: 1.2768 cross entropy : 1.2105 consistency reg : 0.0663\n",
      "840 Step - loss: 1.2933 cross entropy : 1.2295 consistency reg : 0.0638\n",
      "860 Step - loss: 1.3116 cross entropy : 1.2376 consistency reg : 0.0740\n",
      "880 Step - loss: 1.2738 cross entropy : 1.2011 consistency reg : 0.0727\n",
      "900 Step - loss: 1.3206 cross entropy : 1.2432 consistency reg : 0.0773\n",
      "920 Step - loss: 1.2778 cross entropy : 1.1983 consistency reg : 0.0795\n",
      "940 Step - loss: 1.2162 cross entropy : 1.1421 consistency reg : 0.0741\n",
      "960 Step - loss: 1.2685 cross entropy : 1.1768 consistency reg : 0.0916\n",
      "980 Step - loss: 1.2266 cross entropy : 1.1432 consistency reg : 0.0834\n",
      "1000 Step - loss: 1.2380 cross entropy : 1.1494 consistency reg : 0.0886\n",
      "1020 Step - loss: 1.2333 cross entropy : 1.1486 consistency reg : 0.0847\n",
      "1040 Step - loss: 1.2223 cross entropy : 1.1214 consistency reg : 0.1009\n",
      "1060 Step - loss: 1.1916 cross entropy : 1.1023 consistency reg : 0.0894\n",
      "1080 Step - loss: 1.2149 cross entropy : 1.1110 consistency reg : 0.1039\n",
      "1100 Step - loss: 1.1929 cross entropy : 1.0991 consistency reg : 0.0938\n",
      "1120 Step - loss: 1.1985 cross entropy : 1.0833 consistency reg : 0.1152\n",
      "1140 Step - loss: 1.1698 cross entropy : 1.0631 consistency reg : 0.1067\n",
      "1160 Step - loss: 1.1628 cross entropy : 1.0580 consistency reg : 0.1048\n",
      "1180 Step - loss: 1.1797 cross entropy : 1.0532 consistency reg : 0.1265\n",
      "1200 Step - loss: 1.1941 cross entropy : 1.0708 consistency reg : 0.1233\n",
      "1220 Step - loss: 1.1767 cross entropy : 1.0258 consistency reg : 0.1510\n",
      "1240 Step - loss: 1.1349 cross entropy : 1.0169 consistency reg : 0.1180\n",
      "1260 Step - loss: 1.1702 cross entropy : 1.0494 consistency reg : 0.1208\n",
      "1280 Step - loss: 1.1184 cross entropy : 0.9984 consistency reg : 0.1200\n",
      "1300 Step - loss: 1.1622 cross entropy : 1.0021 consistency reg : 0.1601\n",
      "1320 Step - loss: 1.1846 cross entropy : 1.0567 consistency reg : 0.1279\n",
      "1340 Step - loss: 1.1153 cross entropy : 0.9795 consistency reg : 0.1358\n",
      "1360 Step - loss: 1.1294 cross entropy : 1.0068 consistency reg : 0.1226\n",
      "1380 Step - loss: 1.1296 cross entropy : 0.9783 consistency reg : 0.1513\n",
      "1400 Step - loss: 1.1114 cross entropy : 0.9667 consistency reg : 0.1447\n",
      "1420 Step - loss: 1.1491 cross entropy : 1.0034 consistency reg : 0.1457\n",
      "1440 Step - loss: 1.1071 cross entropy : 0.9440 consistency reg : 0.1631\n",
      "1460 Step - loss: 1.1475 cross entropy : 0.9645 consistency reg : 0.1830\n",
      "1480 Step - loss: 1.0202 cross entropy : 0.8808 consistency reg : 0.1394\n",
      "1500 Step - loss: 1.1104 cross entropy : 0.9601 consistency reg : 0.1503\n",
      "1520 Step - loss: 1.0477 cross entropy : 0.8840 consistency reg : 0.1637\n",
      "1540 Step - loss: 1.0852 cross entropy : 0.9049 consistency reg : 0.1803\n",
      "1560 Step - loss: 1.0969 cross entropy : 0.9244 consistency reg : 0.1725\n",
      "1580 Step - loss: 1.0347 cross entropy : 0.8667 consistency reg : 0.1680\n",
      "1600 Step - loss: 1.1101 cross entropy : 0.9132 consistency reg : 0.1968\n",
      "1620 Step - loss: 1.0509 cross entropy : 0.8717 consistency reg : 0.1792\n",
      "1640 Step - loss: 1.0163 cross entropy : 0.8506 consistency reg : 0.1656\n",
      "1660 Step - loss: 1.0607 cross entropy : 0.8740 consistency reg : 0.1867\n",
      "1680 Step - loss: 1.0452 cross entropy : 0.8601 consistency reg : 0.1851\n",
      "1700 Step - loss: 1.0573 cross entropy : 0.8323 consistency reg : 0.2249\n",
      "1720 Step - loss: 1.0738 cross entropy : 0.8543 consistency reg : 0.2194\n",
      "1740 Step - loss: 1.0140 cross entropy : 0.8625 consistency reg : 0.1515\n",
      "1760 Step - loss: 1.0239 cross entropy : 0.8275 consistency reg : 0.1964\n",
      "1780 Step - loss: 1.0328 cross entropy : 0.8381 consistency reg : 0.1948\n",
      "1800 Step - loss: 1.0229 cross entropy : 0.8145 consistency reg : 0.2083\n",
      "1820 Step - loss: 1.0714 cross entropy : 0.8237 consistency reg : 0.2477\n",
      "1840 Step - loss: 0.9674 cross entropy : 0.7991 consistency reg : 0.1682\n",
      "1860 Step - loss: 0.9745 cross entropy : 0.8005 consistency reg : 0.1740\n",
      "1880 Step - loss: 0.9865 cross entropy : 0.7904 consistency reg : 0.1960\n",
      "1900 Step - loss: 0.9931 cross entropy : 0.7525 consistency reg : 0.2405\n",
      "1920 Step - loss: 1.0170 cross entropy : 0.7973 consistency reg : 0.2197\n",
      "1940 Step - loss: 0.9970 cross entropy : 0.7742 consistency reg : 0.2227\n",
      "1960 Step - loss: 0.9872 cross entropy : 0.7883 consistency reg : 0.1989\n",
      "1980 Step - loss: 0.9342 cross entropy : 0.7289 consistency reg : 0.2053\n",
      "Training complete in 2m 48s\n"
     ]
    }
   ],
   "source": [
    "train_consistency_regularization(args, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9a82a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        corrects = 0\n",
    "        total = 0\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(args.device)\n",
    "            targets = targets.to(args.device, dtype=torch.long)\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # output 중 최대값의 위치에 해당하는 class로 예측 수행\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # batch별 정답 개수를 축적함\n",
    "            corrects += torch.sum(preds == targets.data)\n",
    "            total += targets.size(0)\n",
    "\n",
    "    test_acc = corrects.double() / total\n",
    "    print('Testing Acc: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f54c8aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Acc: 0.3873\n"
     ]
    }
   ],
   "source": [
    "test(args, model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
